---
title: "Exploring RNA-seq data from Gierlinski, Schurch and Barton"
author: "Mustafa Abu El-Qumsan & Jacques van Helden"
date: '`r Sys.Date()`'
output:
  html_document:
    fig_caption: yes
    highlight: zenburn
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: true
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
  word_document: default
bibliography: bibliography_rna-seq.bib
---

* * * * * * 

## Parameters for the execution of this tutorial

```{r knitr setup, include=FALSE,  eval=TRUE, echo=FALSE, warning=FALSE}
library(knitr)
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, cache=FALSE, message=FALSE, warning=FALSE, comment = "")
```



```{r execution parameters}

message("specifying parameters")

## The variable dir.base should be adapted to indicate the path of the
## rna-seq_replicate_analysis clone on your computer. 
## All other directories and files are defined relative to this base directory. 
dir.base <- "~/rna-seq_replicate_analysis"
setwd(dir.base)


## Load required libraries
source(file.path(dir.base, 'R-scripts/install_required_libraries.R'))

## Define and check output directories
dir.results <- file.path(dir.base, "results")
dir.create(dir.results, showWarnings = FALSE, recursive = TRUE)
dir.figures <- file.path(dir.base, "figures")
dir.create(dir.figures, showWarnings = FALSE, recursive = TRUE)



## Display and analysis parameters
epsilon <- 1/4

```



## Introduction


The goal of this tutorial is to get familiar with RNA-seq data: 

- load a table of read counts per gene;
- do some exploratory statistics about these counts per gene (distributions, histograms, ...)




### Data source

The 48-replicate yeast study was obtained from Goeff Barton's group publications [@Gierlinski:2015jt, @Schurch:2015vk].

### Methods

Fang and coworkers [-@Fang:2012fg] compared different statistical models and tests (Poisson, Fisher, negative binomial, ...) for the detection of differentially expressed genes. 

#### Full original datasets (beware, there are 672 fastq files !)

Data shared at ENA: <https://figshare.com/articles/Metadata_for_a_highly_replicated_two_condition_yeast_RNAseq_experiment_/1416210>

Fastq files: <http://www.ebi.ac.uk/ena/data/view/ERP004763>

#### Read count tables

- Wild-type yeast strains (WT): <https://dx.doi.org/10.6084/m9.figshare.1425503>
- Snf2 mutant: <https://dx.doi.org/10.6084/m9.figshare.1425502>


## Data loading

We will now load the two read count tables (WT and Snf2 mutants respectively).

```{r data loading}
message("Loading data")

## Load two tables containing the counts per gene for the wild-type (WT) and for the mutant (Snf2) strains, respectively. 
dir.counts <- file.path(dir.base, "data/counts_per_gene")
counts.wt <- read.delim(file=file.path(dir.counts, "WT_raw.tsv"), row.names = 1)
names(counts.wt) <- paste(sep=".", "WT", 1:ncol(counts.wt))

counts.snf2 <- read.delim(file=file.path(dir.counts, "Snf2_raw.tsv"), row.names = 1)
names(counts.snf2) <- paste(sep=".", "SNF2", 1:ncol(counts.snf2))

sample.groups <-c("WT", "Snf2")

message("Data loaded")


```

## Data exploration

We will first check the dimension of the two tables.

```{r}
message("Merging WT and SNF2 counts into a single table.")

## count the number of genes (rows) in the two data tables.
dim(counts.wt)
dim(counts.snf2)

## Check if the row names (gene names) are the same in the two data tables
genes.wt <- row.names(counts.wt)
genes.snf2 <- row.names(counts.snf2)

## Count the number of different genes (should give zero)
if (sum(genes.wt != genes.snf2) > 0) {
  stop("The WT and Snf2 tables do not contain the same number of rows")
}
## OK, it gives 0. 

## Merge the two count tables in a single table
counts <- cbind(counts.wt, counts.snf2)

## Prepare a sample description table
## THIS DOES NOT WORK BECAUSE THE COUNT TABLE CONTAINS 96 columns, i.e. one column per sample, WHEREAS THE sample mapping table contains 7 technical replicates per sample. 
## sample.descriptions <- read.delim(file.path(dir.base, "data/ERP004763_sample_mapping.tsv"), row.names=1)

## Sample description table
sample.description <- data.frame(
  row.names = names(counts),
  genotype = c(rep("WT", times=ncol(counts.wt)), 
               rep("SNF2", times=ncol(counts.snf2))),
  replicate = c(1:48, 1:48)
  )

## Associate one specific color to each genotype
color.code <- c("WT"="#BBBBDD", "SNF2"="#FFBBBB")
sample.description$color <- color.code[sample.description$genotype]

```


The tables *counts.wt* and *counts.snf2* contain the read counts per gene for the Wild-Type  and the Snf2 strains, respectively. Each table contains one row per gene and one column per sample. 


| Genotype |  Rows | Columns |
|----------|-------|---------|
| WT | `r nrow(counts.wt)` |  `r ncol(counts.wt)` |
| Snf2 | `r nrow(counts.snf2)` |  `r ncol(counts.snf2)` |
| All | `r nrow(counts)` | `r ncol(counts)` |
| | |


## Summary of counts per sample (column)

We could use the function *summary()* to compute descriptie statistics for each column of the count table. 

```{r summary of  the counts}
## summary(counts) ## This takes too much place for a report
```

We can immediately draw some observations from these summaries. 

- **min**: in each column, the mean is 0. This corresponds to genes that were undetected: apparently these genes are either not transcribed, or they are transcribed at a weak level so that they were not sequenced in the library. 

- **Mean**: in this experiment the mean counts per gene are of the order of 1000 counts per gene for each biological sample. 

- **max* The max count is immensely larger than the mean counts, and this is true for each column. This observation is very general with RNA-seq data: in all the cases I have studied so far, we detect a very few genes having a huge number of counts. 

- **median** is the count value that splits the sorted values in two. Half of the values of a column are smaller than or equal to the median. 


## Number of undetected genes per sample

We would like to count the number of undetected genes per sample, i.e. genes having zero counts. 

```{r undetected_genes}


## Count the number of undetected genes for each sample
message("Counting undetected genes per sample.")
undetected.genes.per.sample <- apply(counts == 0, 2, sum)

## Count the number of samples where each gene has not been detected
message("Counting undetected samples per gene.")
undetected.samples.per.gene <- apply(counts == 0, 1, sum)
hist(undetected.samples.per.gene, 
     breaks=0:ncol(counts), col="grey", ylim=c(0,300),
     main="Samples with zero counts per gene",
     xlab="Number of samples with 0 counts",
     ylab="Number of genes (truncated axis)"
  )

## JvH: THIS IS NOT WORKING: THE LOOP IS USED, BUT THE PLOTS ARE NOT GENERATED. TO CHECK LATER
for (j in 1:ncol(counts)){
  message("PROBLEM: no ggplot for sample nb ", j)
   # ggplot(counts, aes(x = counts[,j])) + geom_histogram(fill = "#525252", binwidth = 2000)
}

```


```{r detected samples per gene and group}

message("Comparing the number of samples per gene between groups (WT vs SNF2)")

## Compare the number of samples where each gene was detected in the two groups
detected.samples.per.gene.wt <- apply(counts.wt >= 1, 1, sum)
detected.samples.per.gene.snf2 <- apply(counts.snf2 >= 1, 1, sum)
plot(detected.samples.per.gene.wt, detected.samples.per.gene.snf2,
     main="detected genes in WT versus Snf2 samples",
     xlab="WT samples with zero counts",
     ylab="SNf2 samples with zero counts",
     panel.first=grid(col="blue"))
abline(a=0, b=1, col="darkgreen")

## Highlight the Snf2 gene on the plot
points(detected.samples.per.gene.wt["yor290c"], 
       detected.samples.per.gene.snf2["yor290c"], col="red")

```


## Compute the log-transformed counts


RNA-seq count data are characterized by a very wide range of values, with small count numbers of the majority of the genes, and a small number of genes having hundreds fo thousands of counts. For many purposes (visualisation, summarization, ...), it is more relevant to work with log2-transformed counts. 


However, we first must treate a problem: since many genes have 0 counts, the logarithmic transformation would give -Infinite, which creates problems for both computation and display. To circumvent this, we arbitrarily convert the zero counts in a small epsilon value (smaller than 1, so we can distinguish the genes with 1 counts from genes with 0 counts). we will choose an epsilon of 0.01.  

```{r log_counts_computation}
message("Adding epsilon and computing log2-transformed counts")

## Compute log2-transformed counts
## sum(counts == 0) ## Count the number of zero values
counts.epsilon <- counts
counts.epsilon[counts == 0] <- epsilon
counts.log2 <- log2(counts.epsilon)

```


## Box plots


The boxplots below summarizes the distribution of log2-transformed counts per genes for each sample. 

```{r boxplots of log2-transformed count, fig.height=12, fig.width=6, fig.cap="Box plot of the log2-transformed counts per sample. Zero values were converted to an epsilon=1/4 before log2-transformation, and this appear as -2 on the log2-transformed plots. "}
message("Genering boxplots")
for (genotype in c("WT", "SNF2")) {
  boxplot(
    counts.log2[,sample.description$genotype == genotype], 
    horizontal = TRUE, las=1, col=color.code[genotype],
    main=genotype, cex.axis=0.6, cex.label=0.8,
    xlab=paste(sep="", "log2(counts), with epsilon=", epsilon))
}

```



* * * * * * * * *

## Normalization of RNA-seq counts

The total number of counts (``libsum'') can show wide variations from sample to sample, resulting from various sources (biological sample, sequencing, mapping, ...). Before comparing the counts per gene between different samples, it is important to perform a between-sample normalization. 

Several methods have been proposed for this. 

### Counts per million (CPM)

Counts are converted to counts per million by a simple scaling rule. 

We dispose of a raw count table containing `r nrow(counts)` rows (one row per gene) and `r ncol(counts)` columns (one column per sample). 

Let us define $x_{i,j}$ as the count of reads for gene $i$ ($i^{th}$ row) in sample $j$ ($j^{th}$ column). 

We can compute the sum of counts for each sample (column), which is also called the ***libsum***.

$$N_j = \sum_{i=1}^{g}{x_{i,j}}$$

where $g$ is the total number of genes (rows in the table).

The basic way to compute counts per million reads is to divide each count by the libsum of the corresponding sample (column), and multiplying by 1 million for scaling purposes.

$$cpm_{i,j} = \frac{n_{i,j}}{N_j} \cdot 1,000,000$$

### Median-based standardization

A well-known problem with this simple approach is that the libsum can be strongly influenced by outliers, i.e. a handful of genes which are represented by hundreds of millions of reads. 

To circumvent this, one possibility is to use a more robust scaling factor for each sample, for example the median count, or the  $75^{th}$ percentile. 

$$x'_{i,j} = \frac{x_{i,j}}{\text{median}(x_{.j})} \cdot \text{median}(x_{i,j})$$


### When should we use / not use normalized counts?

Normalized log2-transformed counts are very  convenient for display purposes (e.g. to compare the read density between samples or between conditions). 

**However**, they cannot be used for all purposes. Indeed, the normalization procedure converts integer counts into fractional numbers, which do not comply with the probabilistic models underlying manysome of the statistical tests that are performed on these datasets. In particular, the most popular packages for differential  analysis with RNA-seq expect to take as input a table with raw counts. 

The log2 transformation completely modifies the nature of the data, and completely change the scale of the counts. Consequently, log2-transformed data should not be used either as input for the classical differential analysis packages. 

Log2-transformed standardized counts may also be interesting for some clustering or classification purposes, because they will strongly reduce the dynamic range of the data, and the *log2 transformation has a normalizing effect* (this is a general effect) that may contribute to increase the suitedness of the data for some analysis methods (e.g. PCA, clustering, discriminant analysis, ...). We will investigate this very soon. 


# The aim of Normalization 
Is to remove systematic technical effects that occur in the data to ensure that technical bias has minimal impact on the results, and to equates the read cout distribution across samples.

## The Trimmed mean of M-Values (TMM) normalization method


Robinson and Oshlack [@Robinson:2010:pmid20196867] modeled the relationship between  RNA quantities produced by a sample and RNA-seq counts.

$$E[Y_{gk}] = \frac{\mu_{gk} \cdot L_g}{S_k} \cdot N_k$$

$$N_k = \sum_{g=1}^{G} L_{g}$$

$$S_k = \sum_{g=1}^{G} \mu_{gk} L_{g}$$

Counts:

- $Y_{gk}$ = observed read counts for gene $g$ in sample $k$.
- $N_k$ = total number of reads (all genes together) for sample $k$

Actual RNA quantities per sample:

- $E[Y_{gk}]$ = expected read counts for gene $g$ in sample $k$.
- $\mu_{gk}$ = true (but unknown) number of transcripts for gene $g$ in sample $k$.
- $L_g$ = length of the $g^{th}$ gene
- $S_k$ = total RNA (all genes together) for sample $k$.

Quote from Robinson and Oshlack (2010): "The total RNA dataset production, $S_k$ cannot be estimated directly, since we don't know the expression levels and true lengths of every gene." 

Actually we know (approximately) the length o each gene as far as it has been correctly annotated in the genome, but we have no idea about the actual number of RNA molecules produced by each gene ($g$) in a given sample ($k$). 

"One simple yet robust way to estimate the ratio of RNA production uses a weighted trimmed mean of the log expression ratios (trimmed mean of M values (TMM))."

Computation of the $M$ and $A$ values

The $M$ value is the log2-ratio of relative counts for gene $g$ between two samples $k$ and $k'$. 

$$M_{g}= \log_{2}\frac{Y_{gk}/N_{k}}{Y_{gk'}/N_{k'}}$$

$k'$ is the index of the "reference sample". Robinson and Oshlack indicate that for multi-sample analyses, one sample should be selected as reference, and the TMM should be computed for all the other samples, based on this reference. However the authors do not give clues about the way to choose "the" reference sample. In our case, it would seem logial to choose a WT ("normal" yeast strain) as reference, but we still have to choose one among the 48 WT samples. One possiblity is to take a sample that has an average library sum, in order to be in the middle range. 

```{r Robinson-Oshlack M value computation}

## The raw Y are problematic in the formula, but we can use Y + 1 instead (I think this is what Robinson and Oshlack do, but we have to check)
Y <- counts + 1

## Create an empty datagrame to store M values
M <- data.frame(matrix(ncol=ncol(Y), nrow=nrow(Y)))
names(M) <- names(Y)
row.names(M) <- row.names(Y)

## Define the reference sample by selecting the column (sample) at the median 
## place of the sum-ranked columns.
sorted.wt.libsum <- sort(apply(counts.wt, 2, sum))
sorted.wt.names <- names(sorted.wt.libsum)
ref.sample <- sorted.wt.names[length(sorted.wt.libsum)%/%2]
k.prime <- which(names(Y) == ref.sample)

## For the reference sample, use the raw Y
M[, k.prime] <- Y[,k.prime]

## Compute M values
# The Y  variable of Robinson-Oshlack formula corresponds to our "Y" data frame
for (k in 1:ncol(Y)) {
  if (k != k.prime) {
    M[, k] <- log2( (Y[,k]/ sum(Y[,k]))/(Y[,ref.sample] / sum(Y[,ref.sample])))
  }
}
View(M)

## Create an datagram to store A Values 
y <-counts+1
A <-data.frame(matrix(ncol=ncol(y), nrow=nrow(y)))
names(A)<- names(y)
row.names(A)<- row.names(y)

# For the reference smaple, raw Y
A[,k.prime] <- y[,k.prime]
## comute A Values
k<-1
for (k in 1:ncol(y)){
  if(k != k.prime){
    A[ ,k] <- 1/2*log2(y[,k]/ sum(y[,k]) * y[,ref.sample]/sum(y[,ref.sample]))

      }
}

dim(M)
j <- 1
trimming.percent <- c("lower"=3, "upper"=5)
trimming.number <- round(nrow(M) * trimming.percent / 100)
                  
for (j in 1:ncol(M)) {
  M.current.sample.sorted <- sort(unlist(M[,j]), decreasing = FALSE)   
  first.index.to.keep <- (trimming.number["lower"] +1)
  last.index.to.keep <- (length(M.current.sample.sorted) - trimming.number["upper"])
  M.current.sample.trimmed <- M.current.sample.sorted[first.index.to.keep:last.index.to.keep]
  length(M.current.sample.sorted)
  length(M.current.sample.trimmed)
  TMM[j] <- mean(M.current.sample.trimmed)

 TMM_median[j]<- median(M.current.sample.trimmed)
}

  
 
plot(TMM, TMM_median,
     main="Implemented The Trimmed Mean of M-Values Normalisation Method",
     col="#006688", las=1, cex.axis=0.7,
     xlab="TMM",
     ylab="Median of M Values"
    )
grid(col="blue")
abline(col="brown", a=0, b=1) ## Draw a diagonal (slope=1, intercept=0)
abline(h=0, col="black") # Draw an horizontal line to mark X axis
abline(v=0, col="black") # Draw a vertical line to mark Y axis

## Discarding 5% lower and 3% upper value 
  
  

## Exercises for Mustafa
## - compute A value with the same kind of implementation as for M values here
## - compute the trimmed mean of M values (TMM)
##        1. trimming: discard the 5% lower and 3% upper values
##        2. compute the mean of the remaining values
## - compute the median of M values
## -draw a plot that compares the TMM to the median of M values
```


The Absolute expression level ($A$) of gene $g$ in samples $k$ and $k'$ is defined as follows. 

$$A_{g} = \frac {1}{2} \log_{2}(Y_{gk}/N_{k} \cdot Y_{gk}/N_{k`}) for  Y_{g} \neq 0 $$ 


Interpretation: 

- the $M$ value indicates the difference of expression between samples $k$ and $k'$. A positive sign (resp. negative), indicates  that gene $g$ is expressed at a higher (resp. lower) level in sample $k$ than sample $k'$.

We can robustly summarize the observed M values, also we trim both the M values and the A values before taking the weighted average. precision (inverse of the Variance) weights are used to acount for the fact that log fold changes (effectively, a log relative risk) from genes with larger read counts have lower variance on the logarithm scale.

## In practical side to compute TMM for each sample of the 48 replicate data sets.
The Trimmed mean is the average after removing the upper and lower x% of the data. The TMM procedure is doubly trimmed, by log-fold-changes $M_{gk}-{r}$ (sample k relative to sample r for gene) and by absolute intensity $(A_{g})$, by defult we trim the $M_{g}$ values by 30% and the $A_{g}$ values by 5%, 

?sage.test



### Trimmed Mean of M-values (TMM)
- <font color="vlue">Motivation </font> total read count is strongly dependent on a few highly expressed transcripts
- <font color ="blue">Assumption</font> – the majority of genes are not differentially expressed
Method – a trimmed mean is the average after removing the upper and lower x% of the data. The TMM procedure
is doubly trimmed, by log-fold-changes $M_{g}(j, r)$ (sample $j$ relative to sample r for gene $g$)
$$M_{g}(j,r)=/log_{2}(K_{gj}/frac D_{j}) − log_{2}(K_{gr}/frac D_{r})$$
and by absolute intensity $A_{g}(k,r)$
$$A_{g}(j, r) = 1/frac 2 (log_{2}(K_{gj} /D_{j} ) + log_{2}(K_{gr} /frac D_{r}))$$


```{rImplementation of Trimmed Mean of M-Values (TMM) 1}
# k      <- data.frame(matrix(ncol=ncol(counts.wt), nrow=nrow(counts.wt)))
# row.names(k)<-row.names(counts.wt)
# names(k)<-names(counts.wt)
# View(counts.wt[1,2])
# View(k)
# k.prim  <- data.frame(matrix(ncol=ncol(counts.snf2), nrow=nrow(counts.snf2)))
# row.names(k.prim)<-row.names(counts.snf2)
# names(k.prim)<-names(counts.snf2)
# View(k.prim)
# View(k.prim)

libsum.k.wt.per.sample <- apply(counts.wt, 2,sum)
libsum.k.wt.all <- sum(libsum.k.wt.per.sample)
libsum.k.snf2.per.sample <- apply(counts.snf2, 2,sum)
libsum.k.snf2.all <- sum(libsum.k.snf2.per.sample)

# for(j in 1:ncol(counts)) {
#       # for (j in 1:ncol(counts.snf2)){
#      for(i in 1:nrow(counts)){
#             Mg[i] <-log2((counts.wt[i,j]/libsum.k.wt.per.sample[j])/(counts.snf2[i,j]/libsum.k.snf2.per.sample[j]))
#             Ag[i] <- 1/2*(log2((counts.wt[i,j]/libsum.k.wt.all[j])*(counts.snf2[i,j]/libsum.k.snf2.all[j])))
#      # }
#    }
# }


Mg <-function(counts) log2((counts.wt/libsum.k.wt.all)/(counts.snf2/libsum.k.snf2.all))
Ag <- function (counts) 1/2*(log2((counts.wt/libsum.k.wt.all)*(counts.snf2/libsum.k.snf2.all)))

Mg <-apply(as.matrix(counts) ,2, Mg)
Ag <- apply(counts, 2, Ag)        
```

* * * * * * * * *



## Exercises

### Exercise 1. Summary statistics

Compute a table with summary statistics per samples (columns of the count table). In particular, include in this table the following statistics. All statistics should be computed on both the raw counts and log2-transformed counts. 

a. Sum of counts in the sample.
b. Median count per sample.
c. Some illustrative percentiles (0=min, 5, 10, 25, 50, 75, 90, 95, 100=max)
d. Inter-Quartile Range (IQR).
e. Classical statistical estimates (mean, variance, standard deviation).
f. Any other statistics that you moght find relevant.

The result table must contain one row per sample (column of the original count table), and one column  per statistics (points a to f above). 

Hereafter we only display the results for the 10 first samples (columns of the table), for the sake of readibility of the report. However we also calculate the same statistics for each one of the samples (the full table of statistics will be exported in a tab-separated value file).


```{r summary ststistics per sample}
message("Computing summary statistics per sample")

## Prepare a data frame to store the statistics per sample


## Collact sample-wise statistics in a data frame
stats.per.sample <- data.frame(
  mean = apply(counts, 2, mean),
  sd = apply(counts, 2, sd),
  var = apply(counts, 2, var),
  min = apply(counts, 2, min),
  median = apply(counts, 2, median),
  max = apply(counts, 2, max),
  sum = apply(counts, 2, sum),
  IQR=apply(counts, 2, IQR)
)

selected.quantiles <- c(0,0.05,0.1,0.25,0.50,0.75,0.9,0.95,0.99, 1)
sample.quantiles <-  round(t(apply(counts, 2, quantile, selected.quantiles)), digits=1)

colnames(sample.quantiles) <- paste(sep="", "Q", sprintf("%.2f",selected.quantiles))
stats.per.sample <- cbind(stats.per.sample, sample.quantiles)

 #View(stats.per.sample)

## Export the full table with statistics per sample
write.table(x = stats.per.sample, sep="\t", quote = FALSE, col.names=NA,
            file = file.path(dir.results, "stats_per_sample.tab"))


## Print the 10 first rows of the table in markdown format so that it will be nicely displayed in the HTML report
knitr::kable(t(stats.per.sample[1:5,]))

```

### Library size

We will measure the library size for each sample of each genotye, and display the result as boxplot. 

```{r sequencing depth per sample}

# ## counts
# hist(log10(apply(counts, 2, sum)), breaks=50,
#      main="Sequencing depth per sample", col="#DDEEFF", las=1, 
#      xlab="log10(sum of counts per sample)", ylab="Number of samples", xlim=c(6.7,7.2))
# print(counts[1000:1010,])
# dim(counts) ## Check the dimensions of the count table. 
## Result: 7125 rows (genes) and 96 columns (48 samples x 2 genotypes)

## boxplot showing the "library size", also called "sequencing depth" per sample
message("Genering barplots")
for (genotype in c("WT", "SNF2")) {
  barplot(apply(counts[,sample.description$genotype == genotype], 2, sum), 
          horiz = TRUE,
          las=1, cex.names = 0.6,
          col=color.code[genotype],
          main=genotype,
          xlab="Sum of counts (libsum)")
}

```

On the barplot, we notice that some libraries have a lower sequencing depth ("libsum") than other ones. For example, sample SNF2.39 totals `r sum(counts$SNF2.39)` reads whereas SNF2.48 only sums up to `r sum(counts$SNF2.48)`. These two sequencing depths thus differ by a factor two. For this reason, if we want to compute gene-wise statistics, we will need to normalize the counts in order to take into account these sample-wise differences in sequencing depth. 

### Exercise 2. Sample-wise standardization

In this exercise we apply a simplistic method to standardize samples. Note that specialized packages like edgeR and DESeq2 have much more elaborate (and relevant) normalization procedures. This simplistic normalization is only for didactic purposes. 

a. Standardize the count table by scaling all values accoding to the median per sample (do it for the counts and for the log2-transformed counts).
b. Compare the scaling factors that would be applied with libsum and median-based standardization, respectively (draw a plot comparing the scaling factors for each sample). 

```{r median-based sample normalization}
message("Median-based normalization of counts per sample")

## Compute the median per column
median.per.column <- apply(counts, 2, median)

## Compute the global median count for all the samples together
median.count.allsamplestogether <- median(unlist(counts))
#dim(counts)

## Plot a histogram to show the widespread of the median count per sample
hist(median.per.column, breaks=100, col="cyan")
abline(v=median.count.allsamplestogether, col="darkgreen", lwd=3)

## Compute the median-normalized counts for each sample, by taking the ratio between sample-wise median count and global median count
counts.mednorm <- counts.epsilon
for (j in 1:ncol(counts.mednorm)) {
  counts.mednorm[,j] <-   counts.epsilon[,j] / median.per.column[j] * median.count.allsamplestogether
}

## Control: run the commented line before and check that all columns have the same median (they should, since we performed a median-based scaling).
##       print(apply(counts.mednorm, 2, median))

## Compute the log2-transformed counts after median-based standardization
counts.mednorm.log2 <- log2(counts.mednorm)

```
The boxplots highlight the effect of median-based normalization: after normalization, all samples have the same median. 

#### Boxplots of sample-wise standardized counts (log2-transformed)

```{r boxplots of log2-transformed median-based normalized count, fig.height=12, fig.width=8, fig.cap="Box plot of the  median-based normalized and log2-Introduction to differential gene expression analysis using RNA-seqtransformed counts. Zero values were converted to an epsilon=1/4 before log2-transformation, and this appear as -2 on the log2-transformed plots. "}

message("Genering boxplots for median-based log2-transformed counts")
for (genotype in c("WT", "SNF2")) {
  boxplot(
    counts.mednorm.log2[,sample.description$genotype == genotype], 
    horizontal = TRUE, las=1, col=color.code[genotype],
    main=genotype, cex.axis=0.6, cex.label=0.8, 
    xlab=paste(sep="", "median-based normalized log2(counts), with epsilon=", epsilon))
}

```

### Exercise 3. Normalizing effect of the log2 transformation

Plot an histogram of the raw counts (all the table) versus log2-transformed median-based normalized counts.


```{r plot histograms of log2-transformed versus raw counts, fig.width=12, fig.height=5}
message("Distribution of count values, before and after normalization and log2")

par(mfrow=c(1,2)) 
hist(unlist(counts), 
     breaks=20000, xlim=c(0,5000), 
     main="Raw counts", 
     xlab="Read counts per gene (truncated axis)", 
     ylab="Number of genes in all samples", 
     col="#DDBBFF", las=1,cex.axis=0.7)
hist(unlist(counts.mednorm.log2),  
     main="median-norm. log2(counts)", 
     xlab="median-norm. log2(counts) per gene", 
     ylab="Number of genes in all samples", 
     breaks=100, col="#BBDDDD")
par(mfrow=c(1,1))
```


This distribution does not look like any conventional distribution, but this is not surprizing. Indeed, each gene can be considered as a separate object for which we measure the level of expression in 2 different genotypes (wild-type and Snf2 mutant), and which follows its own distribution, with its own mean and dispersion. 

The histograms above can thus be understood as a mixture of `r nrow(counts)` (number of genes) times 2 (number of genotypes) independent distributions, each of which is represented by 48 observations (replicates). 

### Exercise 4. Statistics per gene (rows of the count table)

Based on the standardized log2-transformed counts, compute a table with statistics per gene: for each gene, compute the mean, sd, var, min, median, max, IQR, and a few selected quantiles (0, 0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.95,  1), ...

<font color="red">
**Mustafa: please do the same as what we did above for the samples (columns of the count table), but apply it to genes (rows of the count table).
</font>

```{r stats_per_gene}
message("Computing gene-wise statistics")

## Compute gene-wise statistics
stats.per.gene <- data.frame(
  mean= apply(counts, 1, mean),
  sd= apply(counts, 1, sd),
  var= apply(counts, 1, var),
  min= apply(counts, 1, min),
  median= apply(counts, 1, median),
  max= apply(counts, 1, max),
  IQR= apply(counts, 1, IQR)
  )

## Compute the log2-transformed counts after median-based standardization
counts.mednorm.log2 <- log2(counts.mednorm)

selected.quantiles <- c(0,0.05,0.1,0.25,0.50,0.75,0.9,0.95,0.99,1)

gene.quantiles <-  round(t(apply(counts, 1, quantile, selected.quantiles)), digits=1)
dim(gene.quantiles)
colnames(gene.quantiles) <- paste(sep="", "Q", sprintf("%.2f",selected.quantiles))
colnames(gene.quantiles)
stats.per.gene <- cbind(stats.per.gene, gene.quantiles)
colnames(stats.per.gene)
#  View(stats.per.gene)

## Export the full table with statistics per Gene
write.table(x = stats.per.gene, sep="\t", quote = FALSE, col.names=NA,
            file = file.path(dir.results, "stats_per_gene.tab"))


## Print the 10 first rows of the table in markdown format so that it will be nicely displayed in the HTML report
knitr::kable(t(stats.per.gene[1:10,]))
```



### Exercise 5. Comparison between WT and mutant genotypes

a. Compute the mean counts per gene for each genotype, using the median-based standardized log2-transformed counts. 
b. Draw a plot to compare the mean expression per gene between WT and SNF2 genotypes. 



```{r fig.width=7, fig.height=7, fig.cap="Comparison of mean read counts per gene between the two genotypes. Note that this drawing is very preliminary, since we did not yet normalize the libraries. It is just to get a rough idea. "}

message("Comparing mean counts per gene between the 2 genotypes.")

## Compute mean counts per sample class for each gene
stats.per.gene$WT.mean <- apply(counts.wt, 1, mean)
stats.per.gene$Snf2.mean <- apply(counts.snf2, 1, mean)

## Compute, for each gene, the mean value for each of the two groups
## We define colors with the 6 hexadecimal numbers 
## (ex: http://coolmaxhot.com/graphics/hex-color-palette.htm)
## see also https://terpconnect.umd.edu/~toh/ColorLesson/ for explanation of color perception
plot(stats.per.gene$WT.mean, stats.per.gene$Snf2.mean,
     main="Mean read counts per gene",
     col="#006688", las=1, cex.axis=0.7,
     xlab="WT genotype",
     ylab="Snf2 genotype"
    )
grid(col="blue")
abline(col="brown", a=0, b=1) ## Draw a diagonal (slope=1, intercept=0)
abline(h=0, col="black") # Draw an horizontal line to mark X axis
abline(v=0, col="black") # Draw a vertical line to mark Y axis

```

On this plot, each dot corresponds to one gene, and its horizontal and vertical position  indicate its mean raw counts for WT and Snf2 samples, respectively. 

### Log2 mean values

We can also  draw a plot with the mean of the log2-transformed counts. 
Note that the arithmetic mean of the logarithms is equivalent to the logarithm of the geometric mean. 

$$\frac{1}{n}\sum_{j=1}^{n}{log_2(x_{i,j})} = log_2(\sqrt[n]{\prod_{j=1}^{n}{x_{i,j}}})$$

```{r fig.width=7, fig.height=7, fig.cap="Comparison of mean read log2(counts) per gene between the two genotypes. Note that this drawing is very preliminary, since we did not yet normalize the libraries. It is just to get a rough idea. "}

## Compute mean counts per sample class for each gene
stats.per.gene$WT.mednorm.log2.mean <- apply(counts.mednorm.log2[, sample.description$genotype=="WT"], 1, mean)
stats.per.gene$Snf2.mednorm.log2.mean <- apply(counts.mednorm.log2[, sample.description$genotype=="SNF2"], 1, mean)

plot(stats.per.gene$WT.mednorm.log2.mean, stats.per.gene$Snf2.mednorm.log2.mean,
     main="Mean log2 median-norm. counts per gene",
     col="#008844",
     xlab="WT; log2(counts)",
     ylab="Snf2 mutant; log2(counts)"
    )
abline(a=0,b=1, col="#FF0066", lwd=2)
abline(v=0, col="black")
abline(h=0, col="black")
grid(col="blue")

```


## Next steps


1. Do the exercises above (exploration of the data)
2. Learn to use the *DESeq2* and *edgeR* packages by using the Vignettes. 
3. Run DESeq2 and edgeR on the yeast dataset, in order to detect differentially expressed genes between WT and Snf2. 
4. After that, you will analyse the robustness of the lists of differentially expressed genes by running resampling approaches (bootstrap, jacknife, subsampling). 

We will now create a DGEList object to hold our read counts. DGEList is an example of the custom task-specific structure that are frequently used in Bioconductor to make analyses easier.
**DGEList()** is the function that converts the count matrix into an edgeR object. BUT Firstly:- we must have a group variables that tells edgeR which samples belong to which group.

```{r DGEList_example}
## Cast the count table into an object that can be used by edgeR. This object contains the count table + some additional attributes describing the samples
dgList <- DGEList(counts=counts , genes = rownames(counts)) 

# print(dgList) ## Just for debugging, not to print in the output
# dgList$samples
head(dgList$counts[1:5])
head(dgList$genes[1:5,])
#dgList$genes
#names(cds)
attributes(dgList)
#cds$samples ## Basic 

## Check that the cds$counts contains the same info as in your original count table
dgList$counts[1000:1010, 1:5]
print(dgList$counts[1:10, 1:5])
dim(counts)
dim(dgList$counts)
dim(dgList) ## For convenience, the DGEList object has a method "dim" that returns the dimension of the count field > gives the same result as cds$count)


## I think you
##sum(cds$all.zeros)

```


###  Filtering out undetected genes


#### Why we will do filtering out undetected genes?
in view of these genes could not adequately represented or not have enough reads to contribute to analysis. in addition that the low count reads it would be impossible to detecte differential expression.  

**Motivation: ** now we will apply a filter to discard genes that remain undetected in almost all the samples.

**Explanation: ** In our dataset we have approximatly 7125 Genes. However, many of them will not be expressed, or will not be represented by enough reads to contribute to the analysis. Removing these genes means that we ultimately have fewer tests to perform, thereby we will reduce the problems associated with multiple testing by using **cpm** (= **Counts per million**), whereas we will retain only those genes that are repersented at least 1 cpm reads in our two groups for gene "WT" and "SNF2" (MAEQ, Summarized from ^[@Rueda:2015ut]).

#### How we will do filter out the low count reads?
By keep only those genes that have at least 1 or 2 read per million in at least 96 samples.and thence we can calcluate the Nomalization factors which correct for the different composition of the samples. the effective Library sizes are then the product of actual Library sizes and these factors.

**Procedure: ** starting from the raw count table, 

1. We compute a Boolean table named `countcheck`, which indicates whether each gene (rows) is or not detected in each sample (column). Our criterion for considering a gene as detected or not is to have at least one count for this gene in this sample. 

2. Compute the marginal sums on each row of the Boolean `countcheck` table, and store it in a Boolean vector named `countcheck.sum`, which indicates, for each gene, the number of samples in which it has been detected. 

3. We plot an histogram showing the distribution of the number of samples where each gene is detected.

4. We compute a Boolean vector indicating whether each gene has been detected in at least 2 samples. We discard all the genes that fail to pass this criterion. 

```{r}
## Compute counts per million reads
countsPerMillion <- cpm(dgList)
summary(countsPerMillion)[1:10]

## By definition, th counts per million should bring the sum of counts to precisely 1 million, for each sample. We can check this by running apply(sum) on the CPM table.
summation = apply(countsPerMillion, 2, sum)

# The total number of values in the count table
nb.values <- nrow(countsPerMillion) * ncol(countsPerMillion)
print(paste("Total number of values = ", nb.values))

## Count how many measurements axceed 1 count per million (we will filter our the measurements having less than 1 cpm, because these genes are barely detectable or absent)
countcheck <- countsPerMillion > 1 # Build a data frame indicating, for each measurement in the count table, whether it is above 1 CPM
# View(countcheck)
countcheck.sum <- sum (countsPerMillion > 1)
print(paste("Number of values > 1CPM: ", countcheck.sum)) # the total number of values > 1 CPM


print(paste(sep="", "Percent of values > 1CPM: ", round(digits=1, 100*countcheck.sum/nb.values), "%")) # the total number of values > 1 CPM


## show the process from counts to filtered genes

Raw.count.gene = kable(counts[1:10, ], caption = "Raw counts per gene in each sample.")
count.million.read = kable(countsPerMillion[1:10, ], caption = "Counts per million reads (normalized counts).")
counts.million.read.normalized = kable(countcheck[1:10, ], caption = "Counts per million reads (normalized counts).")
  
## For each gene (row), count the sum of rows of the countcheck, which indicate the number of samples in which this gene was detected above 1 CPM. 
detected.samples.per.gene <- rowSums(countcheck)
hist(detected.samples.per.gene, breaks=0:ncol(countcheck),
     main="Gene detection in samples", col="#FFBBDD",
     xlab="Number of samples where a gene is detected (counts >= 0)",
     ylab="Number of genes")

## We will now suppress all the genes that were detected in less than 2 samples.

## Define a Boolean vector indicating, for each gene, whether we want to keep it or not
keep.Boolean <- detected.samples.per.gene >= 2
table(keep.Boolean)

## Define a vector with the indices of the genes to keep
keep <- which(rowSums(countcheck) >= 2)
print(keep[1:10])

print(paste("Initial number of genes: ", nrow(countcheck)))
print(paste("Number of kept genes: ", sum(keep.Boolean))) ## Count the number of genes that will be kept
print(paste("Number of kept genes: ", length(keep))) ## Count the number of genes that will be kept
print(paste("Percent of kept genes: ", round(digits=1, 100*length(keep)/nrow(countcheck)))) ## Count the number of genes that will be kept


## Apply the filter to the dgList: select the genes that were TRUE in the "kept" vector.
dgList<-dgList[names(keep),]
dgList
summary(cpm(dgList))[1:10]
no.true.gene = apply(cpm(dgList), 2, sum)



#cds <- cds[rowSums(1e+06 * cds$counts/expandAsMatrix(cds$samples$lib.size, dim(cds)) > 1) >= 3, ]
#dim(cds)
#cds <- calcNormFactors(cds)
#cds$samples
#cds$counts
#cds$samples$lib.size * cds$samples$norm.factors
```    

#### What ?

What do we observe ? What do we conclude (answer the original question of the motivation) ?


*Interpretation of the result: ** Among the XXX genes in our orginal count table, XXX (YYY%) were filtered out because they wer detected in less than 2 samples. 

## NORMALISATION by unsing edgeR package

It is important to Normalise RNA-Seq both within and between samples, edgeR implements the trimmed mean of M-Values (TMM) method.


```{r NORMALISATION by unsing edgR packege}
# calcNormFactors
dgList<- calcNormFactors(dgList, method = "TMM")
dgList$count
attributes(dgList)
# kable(dgList[1:1,1:1 ])
sample.description$replicate

# tcc <- new("TCC", dgList$count, sample.description$replicate)
# 
# tcc <- calcNormFactors(tcc, norm.method = "tmm", test.method = "edger", iteration = 3, FDR = 0.1, floorPDEG = 0.05)
# 
# tcc<-estimateDE(tcc, test.method = "edger", FDR = 0.1)
# result <- getResult(tcc, sort = TRUE)
# result[1:5,]
# 

                           
 

```

Framework
- $K_{gj}$ : observed count for gene $g$ in sample (library) $j$
- $G$ : number of genes
- $$D_{j} =\Sigma_{g=1}{G} K_{gj}$$: total number of reads for sample $j$
- $N$ : number of samples in the experiment
- $C_{j}$ : normalization factor associated with sample $j$

### Total read count normalization

<fontcolor = "blue"> Motivation </font> – scaling to library size as a form of normalization makes intuitive sense, given it is expected that sequencing a sample to half the depth will give, on average, half the number of reads mapping to each gene.
<font color = "blue">Assumption </font>– read counts are proportional to expression level and sequencing depth.
<font color = "bule">Method </font> – divide transcript read count by total number of reads and rescale the factors to counts per million

$$C_{j}= /frac{10}{D_{j}}$$

```{r Implementes - count per million library sum-based normalization method}


## Compute CPMS by deividing each column by its sum
cpm <- data.frame(matrix(ncol=ncol(counts), nrow=nrow(counts))) ## Instantiate a CPM data frame with NA values
row.names(cpm) <- row.names(counts)
names(cpm) <- names(counts)
# View(cpm)

## Compute the normalized counts (CPM) for each sample (column)
for (j in 1:ncol(cpm) ){
  cpm[,j] <- 1e+6 * counts[,j] / sum(counts[,j])
}

# View(cpm)

## Validation: check the sum of columns. Each column should sum up to exactly 1 million, by definition
apply(cpm, 2, sum)
## OK, this works !

```


<<<<<<< HEAD
```{rImplementation of Trimmed Mean of M-Values (TMM)}
# k      <- data.frame(matrix(ncol=ncol(counts.wt), nrow=nrow(counts.wt)))
# row.names(k)<-row.names(counts.wt)
# names(k)<-names(counts.wt)
# View(counts.wt[1,2])
# View(k)
# k.prim  <- data.frame(matrix(ncol=ncol(counts.snf2), nrow=nrow(counts.snf2)))
# row.names(k.prim)<-row.names(counts.snf2)
# names(k.prim)<-names(counts.snf2)
# View(k.prim)
# View(k.prim)

libsum.k.wt.per.sample <- apply(counts.wt, 2,sum)
libsum.k.wt.all <- sum(libsum.k.wt.per.sample)
libsum.k.snf2.per.sample <- apply(counts.snf2, 2,sum)
libsum.k.snf2.all <- sum(libsum.k.snf2.per.sample)

# for(j in 1:ncol(counts)) {
#       # for (j in 1:ncol(counts.snf2)){
#      for(i in 1:nrow(counts)){
#             Mg[i] <-log2((counts.wt[i,j]/libsum.k.wt.per.sample[j])/(counts.snf2[i,j]/libsum.k.snf2.per.sample[j]))
#             Ag[i] <- 1/2*(log2((counts.wt[i,j]/libsum.k.wt.all[j])*(counts.snf2[i,j]/libsum.k.snf2.all[j])))
#      # }
#    }
# }


Mg <-function(counts) log2((counts.wt/libsum.k.wt.all)/(counts.snf2/libsum.k.snf2.all))
Ag <- function (counts) 1/2*(log2((counts.wt/libsum.k.wt.all)*(counts.snf2/libsum.k.snf2.all)))

Mg <-apply(as.matrix(counts) ,2, Mg)
Ag <- apply(counts, 2, Ag)   
# plot(Mg,Ag)

```
=======
>>>>>>> 6cb5f4affaf3aa0b2ab33f1fc46d6981e39b6bb4

## Multidimensional scaling



Each sample can be viewed as a vector of `r nrow(dgList)` values (one value per gene). We can thus imagine a "sample  space" made of `r nrow(dgList)` dimensions (one dimension per gene) , where the position of each sample is defined by the normalized counts (CPM) of each gene. 


```{r multidimensional_scaling}
plotMDS(dgList,
        main= "Multidimensional scaling of the samples",
        xlab = "First component",
        ylab = "Second component",
        col="darkblue", cex=0.5
        )
grid()
abline(v=0)
abline(h=0)
```


### Comments (JvH)

You have a matrix with 96 samples and 7200 genes. 
You can consider this table in 2 ways: 

1. You are interested by samples -> each sample (column) will correspond to an "object of interest" and each row (gene) will be one variable that characterizes this object. 

2. Alternatively, you might be interested by genes (rows) and consider each row as one object of interest (gene) which is characterized by its count values in the 96 samples. In this case, the gene is the "object" and the sample is the variable. 

For now on, we will consider that the objects of interest are the samples. Each sample is defined by a vector of 7200 numbers (each column of the count table is a vector), which correspond to 7200 variables (counts per gene).

**Multidimensional scaling** aims at reducing the number of dimensions of a dataset, for various purposes: visualization, variable selection, .... In our case, our 96 samples are characterized by 7200 variables. This is huge ! We have many more variables (genes) than samples. One convenient way to perceive the relationships between samples is to **project** them onto a 2-dimensional space (a plane), for the sake of visualization. This means that you must reduce your 7200 dimensions to no more than 2 dimensions. Various methods  exist for multidimensional scaling: 

- Principal Component Analysis followed by a selection of the 2 first components. 
- **Singular value decomposition** (needs prior computation of a distance matrix between each pair of objects)
- Spring embedding from a distance matrix (needs prior computation of a distance matrix between each pair of objects)

The method plotMDS apparently relies on the SVD method. It does something a bit more complicated, because it computes the distance matrix with a subset of the genes (without actually explaining how this subset is selected), and then applies multidimensional scaling to reduce the  dimensions to a user-specified number (by default, they return only 2 dimensions). 

**Interpretation of the plotMDS result**: in the two first components, the distance between samples is an approximation of the similarity betwen their count profiles per gene (the vectors corresponding to the columns in your count table). 

Interestingly, the first component perfectly separates the WT from the Snf2 samples (the two genotypes). We also notice the presence of some "outliers", i.e. WT samples projecting outside of the WT cluster, and Snf2 samples projected outside of the Snf2 cluster. 

Within each genotype (WT or SNF2, resp.) the samples are highly concentrated in the PC1-PC2 plane. However, for each genotype, a very few samples are discading from the other ones. For example, WT.21, WT.25, WT.28, ... WT.34 for the WT genotype, and SNF2.6, SNF2.13, SNF2.35 for the SNF2 genotype). This might reflect problems with these particular samples, and one might consider them as outliers, and discard them for further analysis. This is a choice that will affect the rest of the analysis. 

# Setting up the Model 

We will specify the design of our Matrix, which describes the setup of our experiment.

```{r setting up the Model}

## Define a vector of the same length as the number of samples, indicating the group of each sample (2 groups: WT and SNF2)

#attributes(counts)
sample.type <- rep("WT", ncol(dgList)) ## Initialize the vector with the same value (WT) for all the entries (this is temporary)

#sample.type 
#colnames(dgList)
sample.type[grep("SNF2",colnames(dgList))]<- "SNF2"

#sample.type ## Check that the 48 first samples are WT, and then 48 of them are  SNF2
table(sample.type)
sample.replicates <- c(1:48, 1:48)
#sample.replicates

designMat <- model.matrix(~sample.replicates + sample.type) ### this need to Check with prof. Jacques.
designMat

```

# Estimating Dispersions 

Now, we will estimate the dispersion parameter for our negative Bionomial Model.
owing to it is difficult to estimate the dispersion accurately for each Gene. therefore we will take a way of **sharing information between genes** BY 

* using a common estimate across all genes.
* Fitting an estimate based on the Mean-Variance trend  across the dataset, such that genes similar abundances have similar 
  variance estimates (Trended dispersion)
* Computing a genewise dispersion (tagwise dispersion).

In edgeR, We use an empirical Bayes method to "Shrink" the genewise dispersion estimates towards the common dispersion (tagwise dispersion).
we must to estimate common or trended dispersion before we can estimate the tagwise dispersion.

```{r Estimating Dispersions}

dgList <- estimateGLMCommonDisp(dgList, design = designMat,   verbose=TRUE)
dgList <- estimateGLMTrendedDisp(dgList, design = designMat,   verbose=TRUE )
dgList <- estimateGLMTagwiseDisp(dgList, design = designMat)

## Now, we can plot the estimates and see how they differ. the Biological coefficient of Variation (BCV) is the square root of the dispersion parameter in the negative binomial Model.
plotBCV(dgList,
        main = "Gene-wise Biological Coefficient of Variation",
        xlab = "Mean per gene",
        ylab = "")

```


### Manual exploration of the method

```{r}

mean.log2.mediannorm.counts.per.gene <- apply(counts.mednorm.log2, 1, mean)
sd.log2.mediannorm.counts.per.gene <- apply(counts.mednorm.log2, 1, sd)
var.log2.mediannorm.counts.per.gene <- apply(counts.mednorm.log2, 1, var)

plot(mean.log2.mediannorm.counts.per.gene, sd.log2.mediannorm.counts.per.gene, col="#BBBBBB", pch=".")

```

Notes:


- since some genes may have zero counts in several samples, the corresponding counts were replaced by an arbitrary epsilon, smaller than 1 -> the log2 is smaller than 0. Consequently, the mean log2-counts can be negative for some genes


****************************************************************
# Differential Expression

Now, we can find our differentially expressed genes. After fitting the Model, we can use **topTags()** function to explore the results, and set thresholds to identify subset of differentially expressed genes.

```{r Differential Expression}

fit <- glmFit(dgList,designMat)
lrt <- glmLRT(fit)
# ?topTags
edgeR_result <- topTags(lrt)

attributes(topTags(lrt))
topTags(lrt)$table        
#write.table(topTags(lrt)$table, file ='edgeR/edgeR_Result.RData',
#      sep ="\t" , quote= FALSE  )
## Finally, we can Plot the Log-fold changes of all the genes, and the highlight those that are differentially expressed
#?decideTests

## This methods returns a 3-values variable indicating whether the gene was significantly up-regulated (1), down-regulated (-1) or non-significant (0)
deGenes <- decideTestsDGE(lrt,p=0.001)
table(deGenes) ## Count the number of under-, similarly- or up-regulated genes 
deGenes.names <-  rownames(lrt)[as.logical(deGenes)]
## Count the number of significant genes

length(deGenes) ## Count the total number of genes
length(deGenes.names) ## Count differentially expressed genes (up-regulated + down-regulated)
length(deGenes.names) / length(deGenes) ##Proportion of differentially expressed genes

plotSmear(lrt, de.tags=deGenes.names, col="grey")
abline(h=c(-1,1),col=2)
abline(h=0,col=1)

```

### Interpretation

The edgeR::plotSmear() method draws n equivalent of the "MA plot'" traditionally used for microarrays. 

<font color="red">Mustafa: find some doc on the MA plot interpretation, for example <https://en.wikipedia.org/wiki/MA_plot></font>

In short: 

- each dot corresponds to one gene;
- the abcissa indicates the average (A) level of expression for the two conditions; 
- the ordinate indicates the log-ratio of expression. 

    - Positive values indicate **up-regulation**, i.e. that the gene is expressed at a higher level in mutant (Snf2) than in wild-type
    - Negative values indicate **down-regulation**: the gene is expressed at a lower level in the mutant than in the wild-type. 
    - the red dots highlight the genes declared significant by edgeR


Note: the smear plot shows strange results: a lot of genes are declared significantly differentially expressed (~3800, i.e. 60% of all genes). This is a huge number of genes, but the plot shows that many of them have a relatively small logFC, which indicates that the mean expression values are quite close between WT and Snf2. 

<font color='red'>How to interpret this ?</font>
- check if the genes declared significant with a small logFG correspond to genes with a very small variance. 

# Estimating Dispersions

The first dispersion type to calculate is the common dispersion. **in the common dispersion stting** each gene gets assigned the same dispersion estimate. The output of the estimation will include the estimate as well as some other elements added to the edgeR Object, cds. 


```{r now we Estimating Dispersions }
## to calculate a commen dispersion, each gene gets assigned the same dispersion estimate 
cds <-DGEList(counts)
cds <- estimateCommonDisp( cds)
names(cds)
cds$common.dispersion 

## the implied standerd deviation are the squere-roots of the Variances
SD <- sd(cds$counts)
sqrt(SD)

cds <- estimateTagwiseDisp(cds, prior.df =0.5319)
summary(cds$tagwise.dispersion)
## More shrinkage / Sqeezing toward the common 
cds <- estimateTagwiseDisp(cds , prior.df = 1.5)
summary(cds$tagwise.dispersion)
```

> "to undesstand what this value means, recall the parameterization for the variance of the negative Bionomial is $$ v(\mu) =  \mu +{\mu^2} \phi $$. for Poisson it's $$v(\mu) =  \mu $$. the implied Standard Deviations are the squere-roots of the variances.  Once the common disprsion is estimated we can estimate the tagwise dispersions. in our senario, each gene will get its own unique dispersion estimate. but the common dispersion is still used in the calcuation. The tagwise disparsions are squeezed toward the common value.** the amount of squeezing** is governed by the paramter prior.n (italic). The higher prior.n, the closer the estimates will be to the common dispersion. The recommended value is the nearest integer to 50/(#samples - #groups). For our yeast dataset that's round(50/(96-2))=1" Quoted from S.Ruddy, 2011.


```{r Plotting the Mean-Variance relationship }
meanVarPlot <- plotMeanVar(cds, show.raw.vars = TRUE ,
                           show.tagwise.vars = TRUE   ,
                           show.binned.common.disp.vars = TRUE ,
                           show.ave.raw.vars = FALSE ,
                           dispersion.method = "qcml" , NBline = TRUE,
                           nbins = 100 ,
                           pch = 16 ,
                           xlab = "Mean Expression (Log10 Scale)" ,
                           ylab = "Variance (Log10 Scale)",
                           main = "Mean- Variance Plot")

```

```{r Visualizing Resulats}
par(mfrow=c(3 ,1))
#hist()
```

```{r Differential Expression of RNA-Seq data at the Gene level by using DESeq package}
## Now, we will creat the central data structure namely CountDataSet by DESeq Package
cds = newCountDataSet(counts,sample.description$replicate)
cds

## Normalisation, we will estimate the effective library size which sometimes also called (Normalisation), the effective library size information calling the Size factors Vector.

# In other words, Now we estimate the **EFFECTIVE LIBRARY SIZE**, Even though there is no relation to normality or a normal distribution, where we can take effective library size information from size factors vector  
cds = estimateSizeFactors(cds)
sizeFactors(cds)

## this method is usefull for visualization our count values which are brought the common scale by dividing each column of the count table by the size factor for this column.
head (counts(cds , normalized=TRUE))

```

## Estimation of variance


in DESeq, the inference relies on the estimation of the typical relationship between the data's viaraince (the data dispersion) and their mean. where is ** the dispersion** can understood as the square of the coefficient of Biological variation, so, if a gene's expression typically differs from replicate to replicate sample by 20%thence gene's dispersion is 0.2 power 2 = 0.04.
we can Note the Variance between counts is the sum of two components: the sample-to-sample variation and uncertinity in measuring a concentration by counting reads.
this emply us to know shot nois or poisson noise, which is the dominating noise source for lowly expressed genes.
The former dominates for highly expressed genes is the sum of both, shot noise and dispersion, is considered in the differential expression inference.$v$ of count vaue is modelled as

**the variance** 

$$v = s \mu +\alpha{s^2} \mu^2$$,

* where $\mu$ is expected normalized count value (estimated by the average normalised count value).
* $s$ is the sive factor for the sample under considaration.
* is the dispertion value for the gene under consideration.


```{r}
cds = estimateDispersions(cds) ## this command to estimate the dispersion.
## this function will performs three steps. 
# * it estimate a dispersion value for each gene.
# * it fits a curve through the estimates.
# * it assignes to each gene a dispersion value. using a choice between the per-gene estimate and the fitted value.

str(fitInfo(cds)) # to allow the intermediate steps. 
plotDispEsts(cds)
fData(cds)[c(1,3,4,5,6,7,8,9,10),]
fData(cds)$disp_pooled
```

# inference: by calling Differential expression
```{r}
#?nbinomTest
#res = nbinomTest(cds, "A","B")
```

# Differential analysis with DESeq2

```{r}


## Define a vector with the conditions per sample
condition <- as.vector(sample.description$genotype)


## Create a DESeqDataSet object from the count table + conditions
deseq2.dds <- DESeqDataSetFromMatrix(
  countData = counts, 
  colData = DataFrame(condition),
  ~ condition)

## Indicate that second condition is the reference condition. 
## If not done, the conditions are considered by alphabetical order, 
## which may be misleading to interpret the log2 fold changes. 
deseq2.dds$condition <- relevel(deseq2.dds$condition, ref="WT") 

## Run the differential analysis
deseq2.dds <- DESeq(deseq2.dds)      ## Differential analysis with negbin distrib
deseq2.res <- results(deseq2.dds, independentFiltering=FALSE, pAdjustMethod = "BH")  ## Collect the result table

summary(deseq2.res)

alpha <- 0.05 ## First type error risk
deseq2.result.table <- data.frame(
  "gene.id" = row.names(deseq2.res),
  "mean" = deseq2.res$baseMean,
  "log2FC" = deseq2.res$log2FoldChange,
  "pvalue" = deseq2.res$pvalue,
  "padj" = deseq2.res$padj)
deseq2.result.table$sig = -log10(deseq2.result.table$padj) ## Compute significance index
deseq2.result.table$called.positive <- deseq2.result.table$padj < alpha
table(deseq2.result.table$called.positive)

## Draw a volcano plot
plot(deseq2.result.table$log2FC,-log10(deseq2.result.table$padj), pch=".", panel.first=grid(), col="grey")

## Mark significant genes in red
marked.genes <- !is.na(deseq2.result.table$called.positive) & deseq2.result.table$called.positive
points(deseq2.result.table[marked.genes,c("log2FC", "sig")], col="red", pch=".")

```


For next time (start inquiring on your site):

- multiple testing corrections, including 

    - nominal p-value
    - False Positive Rate versus False Discovery Rate: understand the difference
    - Bonferroni
    - Benjamini-Hochberg

Next time we discuss about the interpretation of the differentially expressed genes with DESeq2. 

# Estimation of variance


In DESeq, the inference relies on the estimation of the typical relationship between the data's viaraince (the data dispersion) and their mean. where is ** the dispersion** can understood as the square of the coefficient of Biological variation, so, if a gene's expression typically differs from replicate to replicate sample by 20%thence gene's dispersion is 0.2 power 2 = 0.04.
we can Note the Variance between counts is the sum of two components: the sample-to-sample variation and uncertinity in measuring a concentration by counting reads.
this emply us to know shot nois or poisson noise, which is the dominating noise source for lowly expressed genes.
The former dominates for highly expressed genes is the sum of both, shot noise and dispersion, is considered in the differential expression inference.$$v of count vaue is modelled as

***** the variance 
$$v = \sm \sum $ m$$($j^{th}$ column),
* where M is expected normalized count value (estimated by the average normalised count value).
* s is the sive factor for the sample under considaration.
* is the dispertion value for the gene under consideration.


```{r}
cds <- estimateDispersions(cds) ## this command to estimate the dispersion.
## this function will performs three steps. 
# * it estimate a dispersion value for each gene.
# * it fits a curve through the estimates.
# * it assignes to each gene a dispersion value. using a choice between the per-gene estimate and the fitted value.

str(fitInfo(cds))
plotDispEsts(cds)
fData(cds)[c(1,3,4,5,6,7,8,9,10),]
fData(cds)$disp_pooled
```



## References
